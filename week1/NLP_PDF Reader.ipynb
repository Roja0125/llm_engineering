{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d73cc-2c8b-49f3-8687-c1bfc443f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize(\"https://arxiv.org/pdf/2106.09685\")- wont run as this needs pypdf library to scrape pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88bfdd8-a1f6-4a97-9eef-625fdfa514f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some prior works\n",
      "even explicitly impose the low-rank constraint when training the original neural network (Sainath\n",
      "et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\n",
      "dak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\n",
      "considers low-rank update to a frozen model for adaptation to downstream tasks . Using language modeling as an example, there\n",
      "are two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\n",
      "et al., 2019; Rebufﬁ et al., 2017; Pfeiffer et al., 2021; R ¨uckl´e et al., 2020) or optimizing some forms\n",
      "of the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\n",
      "Liu et al., 2021). Consider the following thought\n",
      "experiment: if the downstream task were in a different language than the one used for pre-training, retraining\n",
      "the entire model (similar to LoRA with r=dmodel ) could certainly outperform LoRA with a small r.\n",
      "10Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\n",
      "tion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\n",
      "decomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\n",
      "swer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1\u0014i\u00148) is\n",
      "contained in the subspace spanned by top jsingular vectors of UAr=64(for1\u0014j\u001464)?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "# If you get a wordnet error, also run:\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def summarize_pdf(pdf_path, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Summarizes a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        num_sentences (int, optional): The number of sentences in the summary. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the PDF.\n",
    "    \"\"\"\n",
    "    # Extract text from PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Clean and preprocess text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word.lower() for sentence in sentences for word in sentence.split() if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_frequencies = FreqDist(words)\n",
    "\n",
    "    # Score sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in word_frequencies:\n",
    "                if i not in sentence_scores:\n",
    "                    sentence_scores[i] = word_frequencies[word.lower()]\n",
    "                else:\n",
    "                    sentence_scores[i] += word_frequencies[word.lower()]\n",
    "\n",
    "    # Get top n sentences\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Create summary\n",
    "    summary_sentences = [sentences[i] for i in top_sentences]\n",
    "    summary = \" \".join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = \"/Users/rojapalla/Documents/LORA.pdf\"  # Replace with your PDF file path\n",
    "    summary = summarize_pdf(pdf_file)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51729774-cea3-495d-9463-3088ec8045df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rojapalla/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "\n",
      "Using language modeling as an example, there\n",
      "are two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\n",
      "et al., 2019; Rebufﬁ et al., 2017; Pfeiffer et al., 2021; R ¨uckl´e et al., 2020) or optimizing some forms\n",
      "of the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\n",
      "Liu et al., 2021). RoB base(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4\n",
      "RoB base(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2\n",
      "RoB base(AdptD)* 0.3M 87.1\u0006.094.2\u0006.188.5\u00061.160.8\u0006.493.1\u0006.190.2\u0006.071.5\u00062.789.7\u0006.384.4\n",
      "RoB base(AdptD)* 0.9M 87.3\u0006.194.7\u0006.388.4\u0006.162.6\u0006.993.0\u0006.290.6\u0006.075.9\u00062.290.3\u0006.185.4\n",
      "RoB base(LoRA) 0.3M 87.5\u0006.395.1\u0006.289.7\u0006.763.4\u00061.293.3\u0006.390.8\u0006.186.6\u0006.791.5\u0006.287.2\n",
      "RoB large(FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9\n",
      "RoB large(LoRA) 0.8M 90.6\u0006.296.2\u0006.590.9\u00061.268.2\u00061.994.9\u0006.391.6\u0006.187.4\u00062.592.6\u0006.289.0\n",
      "RoB large(AdptP)y 3.0M 90.2\u0006.396.1\u0006.390.2\u0006.768.3\u00061.094.8\u0006.291.9\u0006.183.8\u00062.992.1\u0006.788.4\n",
      "RoB large(AdptP)y 0.8M 90.5\u0006.396.6\u0006.289.7\u00061.267.8\u00062.594.8\u0006.391.7\u0006.280.1\u00062.991.9\u0006.487.9\n",
      "RoB large(AdptH)y 6.0M 89.9\u0006.596.2\u0006.388.7\u00062.966.5\u00064.494.7\u0006.292.1\u0006.183.4\u00061.191.0\u00061.787.8\n",
      "RoB large(AdptH)y 0.8M 90.3\u0006.396.3\u0006.587.7\u00061.766.3\u00062.094.7\u0006.291.5\u0006.172.9\u00062.991.5\u0006.586.4\n",
      "RoB large(LoRA)y 0.8M 90.6\u0006.296.2\u0006.590.2\u00061.068.2\u00061.994.8\u0006.391.6\u0006.285.2\u00061.192.3\u0006.588.6\n",
      "DeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\n",
      "DeB XXL(LoRA) 4.7M 91.9\u0006.296.9\u0006.292.6\u0006.672.4\u00061.196.0\u0006.192.9\u0006.194.9\u0006.493.0\u0006.291.3\n",
      "Table 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\n",
      "GLUE benchmark. Some prior works\n",
      "even explicitly impose the low-rank constraint when training the original neural network (Sainath\n",
      "et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\n",
      "dak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\n",
      "considers low-rank update to a frozen model for adaptation to downstream tasks .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def summarize_pdf(pdf_path, num_sentences=3):\n",
    "    # Read the PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \" \"\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Prepare stopwords and lemmatizer\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Clean and tokenize text\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word.isalpha() and word not in stop_words:\n",
    "                lemma = lemmatizer.lemmatize(word)\n",
    "                words.append(lemma)\n",
    "\n",
    "    # Compute word frequency\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Score sentences\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_words = word_tokenize(sentence.lower())\n",
    "        sentence_score = 0\n",
    "        for word in sentence_words:\n",
    "            if word.isalpha():\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                if word not in stop_words:\n",
    "                    sentence_score += word_freq[word]\n",
    "        sentence_scores[i] = sentence_score\n",
    "\n",
    "    # Pick top scoring sentences\n",
    "    top_indices = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    top_indices.sort()  # Preserve original order\n",
    "\n",
    "    summary = \" \".join([sentences[i] for i in top_indices])\n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = \"/Users/rojapalla/Documents/LORA.pdf\"  # Update as needed\n",
    "    summary = summarize_pdf(pdf_file, num_sentences=3)\n",
    "    print(\"\\nSummary:\\n\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e3cad-cc3e-4805-9aaf-b69b8b2ca32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
